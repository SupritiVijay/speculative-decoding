{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CV0u6rXxl7rk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_q = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
        "model_q = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
        "model_q.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nuI6Pks78Ot",
        "outputId": "00ed8016-9bdc-43f0-d5d0-92220d5bfdce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_p = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "model_p = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "model_p.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j_JrUii78pN",
        "outputId": "7e2078bb-6ad8-4b2a-e7f0-8c0a7caac00d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
              "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
              "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
              "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
              "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
              "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_k_tokens_model_q(tokens, k):\n",
        "  k_tokens = []\n",
        "  k_probs = []\n",
        "  for i in range(k):\n",
        "    outputs = model_q(input_ids=tokens).logits[:,-1,:]\n",
        "    outputs = F.softmax(outputs, dim=-1)  #1, vocab_size\n",
        "    q_index= torch.argmax(outputs, dim=-1)[0].item()\n",
        "    k_tokens.append(q_index)\n",
        "    k_probs.append(outputs[0][q_index].item())\n",
        "    tokens = torch.cat([tokens, torch.tensor([[q_index]], device=tokens.device)], dim=1)\n",
        "  return k_tokens, k_probs"
      ],
      "metadata": {
        "id": "LuDZbl1-78wk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_forward_pass_model_p(tokens_plus_next_k, k):\n",
        "  outputs = model_p(input_ids=tokens_plus_next_k).logits[:,-(k+1):,:] #if we were to predict with 5 tokens and k = 5.\n",
        "  #if we woul not have done k+1 we would have missed 0th of the generated token because eahc logits predicts the next token.\n",
        "  #so for k+1 -> it was taking position k\n",
        "  #and [:, -1,:] predicts a fresh new token given all; in that sense, [:, -2,:] predicts or verifies the last token the small model generated\n",
        "  outputs = F.softmax(outputs, dim=-1)\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "rX2BzBbDDvVI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify(p_probs, k_next_probs, k_next_tokens):\n",
        "  random.seed(0)\n",
        "  p_probs = p_probs.squeeze(0)\n",
        "  accepted_list = []\n",
        "  for index, (q_prob, q_token_id) in enumerate(zip(k_next_probs, k_next_tokens)): #p(x)>=q(x)\n",
        "    distribution = p_probs[index]\n",
        "    p_x = distribution[q_token_id].item()\n",
        "    if q_prob > p_x:\n",
        "      rejection_criteria = 1 - (p_x/q_prob)\n",
        "      r = random.random()\n",
        "      if r > rejection_criteria:\n",
        "        accepted_list.append(q_token_id)\n",
        "      else:\n",
        "        return accepted_list, distribution\n",
        "    else:\n",
        "      accepted_list.append(q_token_id)\n",
        "  return accepted_list, p_probs[-1]"
      ],
      "metadata": {
        "id": "sqZFkQ8xIX1l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(prompt, k = 5):\n",
        "  inputs = tokenizer_q(prompt, return_tensors=\"pt\").to(\"cuda\").input_ids\n",
        "  for _ in range(10):\n",
        "    k_next_tokens, k_next_probs = predict_next_k_tokens_model_q(inputs, k)\n",
        "    tokens_plus_next_k = torch.tensor(k_next_tokens, dtype=torch.long, device='cuda').unsqueeze(0)\n",
        "    tokens_plus_next_k = torch.cat([inputs, tokens_plus_next_k], dim=-1)\n",
        "    p_probs = parallel_forward_pass_model_p(tokens_plus_next_k, k)\n",
        "    accepted_list, distribution_p = verify(p_probs, k_next_probs, k_next_tokens)\n",
        "    next_token_id_p = torch.argmax(distribution_p, dim=-1).item()\n",
        "    accepted_list.append(next_token_id_p)\n",
        "    accepted_list = torch.tensor(accepted_list, dtype=torch.long,  device='cuda').unsqueeze(0)\n",
        "    inputs = torch.cat([inputs, accepted_list], dim=1)\n",
        "    input_list = inputs[0].tolist()\n",
        "    decoded_text = tokenizer_q.decode(input_list)\n",
        "  return decoded_text"
      ],
      "metadata": {
        "id": "VD_8rhPAMYdA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "x = main(\"I look forward to\")\n",
        "time.time() - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2iJ51YZUhwM",
        "outputId": "09c0d16d-6d33-4d92-f601-0e4bab9cdc2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.75476336479187"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(tokenizer_q(x)[\"input_ids\"])\n",
        "num_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atu1YSzEZpga",
        "outputId": "1059348c-e412-490d-cf00-2b02d1c7af7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sOfzSxaUrSc",
        "outputId": "33a54cf7-73cd-4349-ee8a-5ace90414186"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I look forward to your response.\n",
            "\n",
            "Best regards,\n",
            "Emily<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Dear Alex,\n",
            "\n",
            "I hope this message finds you well. I am reaching out to you regarding a recent project I am working on, which involves analyzing data from a large dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "inputs = tokenizer_p(\"I look forward to\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "output_ids = model_p.generate(\n",
        "    inputs.input_ids,\n",
        "    max_new_tokens=num_tokens,\n",
        "    do_sample=False,\n",
        "    eos_token_id=None,\n",
        "    pad_token_id=tokenizer_p.pad_token_id,\n",
        ")\n",
        "\n",
        "decoded_text = tokenizer_p.decode(output_ids[0], skip_special_tokens=False)\n",
        "time.time() - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecPhsZIiX9ik",
        "outputId": "5f90d0db-69db-4591-ee3c-66ecea749f28"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5592286586761475"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLiw85FCZEh3",
        "outputId": "b5ba10ea-33fa-4f12-9827-00055af44981"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I look forward to your thoughts.\n",
            "\n",
            "Best regards,\n",
            "Emily<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Emily is pleased to share that the proposed project on the impact of climate change on the Arctic region has been accepted for funding. She is eager to discuss the project's potential and the implications of the research.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjGmht3-ZJ7H"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}